{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41397a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d04036",
   "metadata": {},
   "source": [
    "## 10-minute PSD on our stations ::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7299fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do without interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c407c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Minutes to Analyze (from previous analysis)\n",
    "# Pull records for those minutes (10 records)\n",
    "# flatten records (60*2597 datapoints for reach record)\n",
    "# Check for outliers\n",
    "# I did this by looking at the individual minutes\n",
    "# Add timestamps to all records\n",
    "# Run psd with calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_record(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from a SNIPE Magnetometer .h5 file.  Cleans the timestamps and puts them into a 1D array.\n",
    "    Puts the magnetometer reading data into a 2D array, where each column represents a second and each row one of the\n",
    "    2597 individual samples for that seconds in chronological order.\n",
    "    The function also captures the timestamp associated with the filename for possible use.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (string): .h5 data file.  Ensure the full path is correct!\n",
    "    \n",
    "    Returns:\n",
    "    - file_timestamp (datetime): Timestamp for file (minute worth of data)\n",
    "    - timestamps (datetime array): Timestamps associated with data 'seconds'\n",
    "    - data (2D float array): Data associated with timestamps\n",
    "    \n",
    "    Note: The function Transposes the data array to make it easier for manipulation.\n",
    "    '''\n",
    "    index = np.array(h5py.File(filename)['timestamps'])\n",
    "    data = np.array(h5py.File(filename)['data']).T # See note above\n",
    "    \n",
    "    timestamps = []\n",
    "    \n",
    "    file_timestamp = datetime.strptime(filename[29:-3], '%Y-%m-%d_%H-%M-%S-%f')\n",
    "    \n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format (the try except was just put in to address bad data in file)\n",
    "        try:\n",
    "            d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            timestamps.append(d)\n",
    "        except:\n",
    "            decoded = decoded_string[:19] + '.' + decoded_string[19:]\n",
    "            d = datetime.strptime(decoded, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            timestamps.append(d)\n",
    "\n",
    "    return file_timestamp, timestamps, data\n",
    "\n",
    "\n",
    "\n",
    "def flatten_data(data):\n",
    "    '''\n",
    "    Takes in a 2D array of time data (seconds, samples) and converts it to one long array (samples).\n",
    "    \n",
    "    Ensure the data is properly transposed!\n",
    "    '''\n",
    "    \n",
    "    return data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime(2024, 7, 26, 17, 30, 0)\n",
    "\n",
    "analysis_start_time = start_time + timedelta(seconds = 130000)\n",
    "analysis_end_time = analysis_start_time + timedelta(minutes = 10)\n",
    "\n",
    "analysis_start_time, analysis_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go through the files in the directory and find filename near\n",
    "directoryEW = './july_data/Ha_EW'\n",
    "directoryNS = './july_data/Ha_NS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2502b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_minute_ts(directory, analysis_start_time):\n",
    "    '''\n",
    "    '''\n",
    "    # To ensure proper ordering of data, first need to run a file type check and sort\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list = sorted(file_list)\n",
    "\n",
    "    # Initilize the data holder array\n",
    "    output = np.zeros(10*60*2597) # 10 minutes, 60 seconds, 2597 samples/second\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 0\n",
    "    \n",
    "    # Set the end time for ten minutes - this can be made into a parameter as well\n",
    "    analysis_end_time = analysis_start_time + timedelta(minutes = 10)\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        # First, extract the file time from the file's name\n",
    "        file_time = datetime.strptime(filename[11:-3], '%Y-%m-%d_%H-%M-%S-%f')\n",
    "        # If the file time is after the analysis_end time, break out of (end) the loop\n",
    "        if file_time > analysis_end_time:\n",
    "            break\n",
    "        # If the file time is before the analysis starts, continue (skip) over it\n",
    "        if file_time < analysis_start_time:\n",
    "            continue\n",
    "        else:\n",
    "            # If the the file is in the analysis time, print the file name for situational awareness\n",
    "            print(f'Includes file: {filename}')\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            # Extract the timestamps and data from the record\n",
    "            file_timestamp, timestamps, data = pull_record(file_path)  \n",
    "\n",
    "            # Flatten data into a single 1D array\n",
    "            data = flatten_data(data)\n",
    "\n",
    "            # For plugging into the np.zeros placeholder array, set the start and end of the array using the counter\n",
    "            low_index = counter * 60 * 2597\n",
    "            high_index = low_index + 60 * 2597\n",
    "\n",
    "            output[low_index:high_index] = data\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} of {len(file_list)} records', end = '\\r')\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 2597\n",
    "start_time = datetime(2024, 7, 26, 17, 30, 0)\n",
    "\n",
    "analysis_start_time = start_time + timedelta(seconds = 132000)\n",
    "analysis_end_time = analysis_start_time + timedelta(minutes = 10)\n",
    "\n",
    "print('Start Time:',analysis_start_time,'| End Time:',analysis_end_time)\n",
    "plt.plot(ten_minute_ts(directoryEW, analysis_start_time),alpha=.5)\n",
    "plt.plot(ten_minute_ts(directoryNS, analysis_start_time), alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27940b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmts_outputEW = ten_minute_ts(directoryEW, analysis_start_time)\n",
    "tmts_outputNS = ten_minute_ts(directoryNS, analysis_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4acc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_EW = np.fft.fft(tmts_outputEW)\n",
    "fft_NS = np.fft.fft(tmts_outputNS)\n",
    "\n",
    "n_EW = len(fft_EW)\n",
    "n_NS = len(fft_NS)\n",
    "\n",
    "psd_EW = (1 / (sample_rate * n_EW)) * np.abs(fft_EW[:n//2])**2\n",
    "psd_EW = np.sqrt(2*psd_EW)\n",
    "\n",
    "psd_NS = (1 / (sample_rate * n_NS)) * np.abs(fft_NS[:n//2])**2\n",
    "psd_NS = np.sqrt(2*psd_NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(psd_EW, alpha = .5)\n",
    "plt.plot(psd_NS, alpha = .5)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee155c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fft frequencies needed for this analysis\n",
    "freqs = np.fft.fftfreq(60*sample_rate, 1/sample_rate)[:(60*sample_rate)//2]\n",
    "calibration_data_file_EW = 'calibrationN749.csv'\n",
    "calibration_data_file_NS = 'calibrationN761.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7102e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snipe_analysis import calibrate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dac9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_psd_EW = calibrate_data(freqs, psd_EW, calibration_data_file_EW)\n",
    "cal_psd_NS = calibrate_data(freqs, psd_NS, calibration_data_file_NS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f5fd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(cal_psd_EW, alpha = .5)\n",
    "plt.plot(cal_psd_NS, alpha = .5)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732249ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cal_psd_EW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    # To ensure proper ordering of data, first need to run a file type check and sort\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list = sorted(file_list)\n",
    "\n",
    "    # Initilize the data holder array\n",
    "    output = np.zeros(10*60*2597) # 10 minutes, 60 seconds, 2597 samples/second\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        file_time = datetime.strptime(filename[11:-3], '%Y-%m-%d_%H-%M-%S-%f')\n",
    "        if file_time > (analysis_end_time):\n",
    "            continue\n",
    "        if file_time < (analysis_start_time):\n",
    "            continue\n",
    "        else:\n",
    "            print(filename)\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            file_timestamp, timestamps, data = pull_record(file_path)  \n",
    "\n",
    "            data = flatten_data(data)\n",
    "\n",
    "            low_index = counter * 60 * 2597\n",
    "            high_index = low_index + 60 * 2597\n",
    "\n",
    "            output[low_index:high_index] = data\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} of {len(file_list)} records', end = '\\r')\n",
    "\n",
    "    plt.plot(output, alpha = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59bda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Minutes to Analyze (from previous analysis)\n",
    "# Pull records for those minutes (10 records)\n",
    "# flatten records (60*2597 datapoints for reach record)\n",
    "# Add timestamps to all records\n",
    "# Interpolate timestamps\n",
    "# Run psd with calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b048a1",
   "metadata": {},
   "source": [
    "## CALIBRATION WORK :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f97373",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data_file1 = 'calibrationN691.csv'\n",
    "cal_data_file2 = 'Coil1Calibration.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec010405",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data = np.loadtxt(cal_data_file1, delimiter = \",\")\n",
    "\n",
    "frequency_calibration = calibration_data[:, 0]  # Frequency values in Hz\n",
    "voltage_calibration = 10**-3 * calibration_data[:, 1]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "phase_calibration = calibration_data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a441352",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.loadtxt(cal_data_file1, delimiter = \",\")\n",
    "b = np.loadtxt(cal_data_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ddb2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snipe_analysis import get_calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179ce372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.45276066e+03, 1.36295890e+03, 7.57373918e+02, 4.20860272e+02,\n",
       "        2.33865154e+02, 1.29955033e+02, 7.22138816e+01, 4.01280703e+01,\n",
       "        2.22985109e+01, 1.23909170e+01, 6.88542948e+00, 3.82612030e+00,\n",
       "        2.12611233e+00, 1.18144577e+00, 6.56510051e-01, 3.64811874e-01,\n",
       "        2.02719979e-01, 1.12648169e-01, 6.25967412e-02, 3.47839830e-02,\n",
       "        1.93288892e-02, 1.07407469e-02, 5.96845700e-03, 3.31657372e-03,\n",
       "        1.84296565e-03, 1.02410580e-03, 5.69078809e-04]),\n",
       " array([1.12287191e-04, 1.61830973e-04, 1.72531087e-04, 1.78879447e-04,\n",
       "        1.82661065e-04, 1.84718472e-04, 1.85721981e-04, 1.86865820e-04,\n",
       "        1.87840177e-04, 1.88606486e-04, 1.88007419e-04, 1.87769617e-04,\n",
       "        1.88239406e-04, 1.87768249e-04, 1.85937531e-04, 1.81875648e-04,\n",
       "        1.71471363e-04, 1.65786353e-04, 1.53167249e-04, 1.46496839e-04,\n",
       "        1.41268730e-04, 1.31100715e-04, 1.16144547e-04, 9.21417451e-05,\n",
       "        6.53369643e-05, 5.52598503e-05, 2.75744396e-05]),\n",
       " array([ 6.05502442e+00,  2.97192733e+00,  1.64379142e+00,  9.30921224e-01,\n",
       "         5.28242626e-01,  3.01530813e-01,  1.79447545e-01,  9.23416814e-02,\n",
       "         5.25027416e-02,  3.03465530e-02,  1.93260061e-02,  1.09963003e-02,\n",
       "         5.08090648e-04, -1.20686687e-02, -3.64920787e-02, -5.01140962e-02,\n",
       "        -1.14917794e-01, -2.31037823e-01, -1.70432148e-01, -2.00414196e-01,\n",
       "        -2.69133893e-01, -3.89753042e-01, -5.06722358e-01, -8.38452737e-01,\n",
       "        -1.02422730e+00, -1.94065432e+00, -3.82077298e+00]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_calibration_data(cal_data_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ba834",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a[:, 0], label = 'a0')\n",
    "plt.plot(a[:, 1], label = 'a1')\n",
    "plt.plot(a[:, 2], label = 'a2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(b[:,0],b[:, 1], label = 'b1')\n",
    "plt.plot(b[:,0],b[:, 2], label = 'b2')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab25fe",
   "metadata": {},
   "source": [
    "1. After the entire dataset is finished determine the mean, std and average difference from mean for the minute.\n",
    "2. Any data that is found to be funky, identify that index.\n",
    "3. Replace that index with the interpolation from either side.\n",
    "4. Give data on outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(truncated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev_cutoff = 2\n",
    "\n",
    "avg = truncated_data.mean()\n",
    "mx = max(truncated_data)\n",
    "mn = min(truncated_data)\n",
    "stddev = np.std(truncated_data)\n",
    "max_diff = max(abs(mx-avg),abs(mn-avg))\n",
    "\n",
    "std_cutoff_low = avg - (stddev_cutoff * stddev)\n",
    "std_cutoff_high = avg + (stddev_cutoff * stddev)\n",
    "\n",
    "outlier_indices = truncated_data[(truncated_data < std_cutoff_low) | (truncated_data > std_cutoff_high)].index\n",
    "\n",
    "truncated_data.loc[outlier_indices] = np.nan\n",
    "\n",
    "print(truncated_data[25:35])\n",
    "\n",
    "# Interpolate the NaN values\n",
    "truncated_interpolated = truncated_data.interpolate()\n",
    "\n",
    "# Fill in any leading or trail zeros (for now simply make the first or last good value)\n",
    "truncated_interpolated = truncated_interpolated.ffill()\n",
    "truncated_interpolated = truncated_interpolated.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcac5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_filter(series, stddev_cutoff):\n",
    "    '''\n",
    "    stddev_cutoff = the number of standard devaitions from mean we consider an outlier\n",
    "    \n",
    "    '''\n",
    "    avg = series.mean()\n",
    "    mx = max(series)\n",
    "    mn = min(series)\n",
    "    stddev = np.std(series)\n",
    "    max_diff = max(abs(mx-avg),abs(mn-avg))\n",
    "\n",
    "    std_cutoff_low = avg - (stddev_cutoff * stddev)\n",
    "    std_cutoff_high = avg + (stddev_cutoff * stddev)\n",
    "\n",
    "    outlier_indices = truncated_data[(truncated_data < std_cutoff_low) | (truncated_data > std_cutoff_high)].index\n",
    "\n",
    "    truncated_data.loc[outlier_indices] = np.nan\n",
    "    \n",
    "    # Interpolate the NaN values\n",
    "    truncated_interpolated = truncated_data.interpolate()\n",
    "\n",
    "    # Fill in any leading or trail zeros (for now simply make the first or last good value)\n",
    "    truncated_interpolated = truncated_interpolated.ffill()\n",
    "    truncated_interpolated = truncated_interpolated.bfill()\n",
    "    \n",
    "    return truncated_interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41649348",
   "metadata": {},
   "source": [
    "## Establish global start and stop times.\n",
    "- Reviewed start and stop times for all experiments, this data can be found summarized in a speadsheet created for all the runs.\n",
    "- Also, checked the overall data integrity and identified a few gaps and others issues in the data.\n",
    "- The determined boundaries are set below, and then a check run for the expected number of records for that period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee771ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime(2024, 7, 26, 17, 30, 0) # Jul 26 @ 1730\n",
    "end_time = datetime(2024, 7, 28, 15, 29, 59) # Jul 28 @ 1530, not inclusive\n",
    "end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total length of records should be {24*60*60 + 79199 + 1}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f708fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_record(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from .h5 file.  Cleans the timestamps and sets it as the index\n",
    "    for a Pandas DataFrame where each column represents a second of data and the rows are the\n",
    "    2597 individual samples for each of those seconds in chronological order.\n",
    "    \n",
    "    Note: The function Transposes the data array to make it easier for manipulation.\n",
    "    '''\n",
    "    index = np.array(h5py.File(filename)['timestamps'])\n",
    "    data = np.array(h5py.File(filename)['data']).T # See note above\n",
    "    \n",
    "    timestamps = []\n",
    "    \n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format (the try except was just put in to address bad data in file)\n",
    "        try:\n",
    "            d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            timestamps.append(d)\n",
    "        except:\n",
    "            decoded = decoded_string[:19] + '.' + decoded_string[19:]\n",
    "            d = datetime.strptime(decoded, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            timestamps.append(d)\n",
    "\n",
    "    return timestamps, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98fe41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interpolated_seconds_data(timestamps, data):\n",
    "    '''\n",
    "    For each second's worth of samples (2597) get the key descriptive data as a downsample.\n",
    "    \n",
    "    Returns the mean, std and max difference from mean for that second - the latter useful\n",
    "    for outlier identification.\n",
    "    '''\n",
    "    # Create the data holder for the output (the average voltage for each second, indexed to a timestamp)\n",
    "    avg_voltage = np.zeros(60) \n",
    "    \n",
    "    # Create the array of rounded timestamps (to the second)\n",
    "    timestamps_floor = [x.replace(microsecond=0) for x in timestamps]   \n",
    "    \n",
    "    # Now convert to seconds\n",
    "    timestamps_floor_int = np.array([dt.timestamp() for dt in timestamps_floor])\n",
    "    # and do the same for the original timestamps\n",
    "    timestamps = np.array([dt.timestamp() for dt in timestamps])\n",
    "        \n",
    "    for i, second in enumerate(data):\n",
    "        avg_voltage[i] = second.mean()\n",
    "        \n",
    "    # Now interpolate!\n",
    "    avg_voltage_interp = np.interp(timestamps_floor_int, timestamps, avg_voltage)\n",
    "        \n",
    "    return timestamps_floor, avg_voltage_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed07653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NON_interpolated_seconds_data(timestamps, data):\n",
    "    '''\n",
    "    For each second's worth of samples (2597) get the key descriptive data as a downsample.\n",
    "    \n",
    "    Returns the mean, std and max difference from mean for that second - the latter useful\n",
    "    for outlier identification.\n",
    "    '''\n",
    "    # Create the data holder for the output (the average voltage for each second, indexed to a timestamp)\n",
    "    avg_voltage = np.zeros(60) \n",
    "        \n",
    "    for i, second in enumerate(data):\n",
    "        avg_voltage[i] = second.mean()\n",
    "    \n",
    "    \n",
    "    return avg_voltage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test \n",
    "test_file = 'test_file.h5'\n",
    "timestamps, data = pull_record(test_file)\n",
    "get_interpolated_seconds_data(timestamps, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "'./july_data/Bu_1',\n",
    " './july_data/Bu_2',\n",
    " './july_data/Bu_3',\n",
    " './july_data/Bu_4',\n",
    " './july_data/Ha_EW',\n",
    " './july_data/Ha_NS',\n",
    " './july_data/Ms_EW',\n",
    " './july_data/Ms_NS',\n",
    " './july_data/Ob_EW',\n",
    " './july_data/Ob_NS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ec40e",
   "metadata": {},
   "source": [
    "Directories to scan:\n",
    "1. [X] Bucknell Mag 1\n",
    "2. [x] Bucknell Mag 2\n",
    "3. [x] Bucknell Mag 3\n",
    "4. [x] Bucknell Mag 4\n",
    "5. [x] Hayward N/S\n",
    "6. [x] Hayward E/W\n",
    "7. [x] Messiah N/S\n",
    "8. [x] Messiah E/W\n",
    "9. [x] Oberlin N/S\n",
    "10. [x] Oberlin E/W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0c1fe",
   "metadata": {},
   "source": [
    "#### I used the following cell to investigate the directories to identify problematic data artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in directories:\n",
    "    # To ensure proper ordering of data, first need to run a file type check\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list = sorted(file_list)\n",
    "\n",
    "    a = int(file_list[0][-15:-13])\n",
    "    for file in file_list[1:]:\n",
    "        if a == 59:\n",
    "            a = -1\n",
    "\n",
    "        b = int(file[-15:-13])\n",
    "        if b != (a + 1):\n",
    "            print(f\"File fail at {directory+'/'+file}\")\n",
    "        a = b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff3ba7",
   "metadata": {},
   "source": [
    "# add more detail:\n",
    "\n",
    "The file fails for Ha_NS, Ms_EW, and Ms_NS, and Bu_2[0,2] are not true fails, as the error crossed over the millisecond.  Also Bu_4[2].\n",
    "\n",
    "There is a ~5 minute gap in the data for Bu_1, ~10 minute gap for Bu_2[1].  Mulitple small gaps for Bu_3 and  Bu_4 which conincide in time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9f990",
   "metadata": {},
   "source": [
    "### Errors and actions:\n",
    "1. Bucknell Mag2 - ValueError: time data '2024-07-27 20:13:44000000' does not match format '%Y-%m-%d %H:%M:%S.%f'; at file 1603\n",
    "- Resolved!  Deleted bad data files and adjusted pull record to account for single corrupted file.\n",
    "2. Oberlin EW - OSError: Unable to open file (bad object header version number); at file 1458\n",
    "- deleted snipe_hunt_2024-07-27_15-44-32-250463\n",
    "3. Oberlin NS - OSError: Unable to open file (bad object header version number); at file 1460\n",
    "- deleted snipe_hunt_2024-07-27_15-47-18-730810"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a7feb",
   "metadata": {},
   "source": [
    "## Single Directory, Interpolated Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the below, we combine indexed series instead of adding the index at the end\n",
    "\n",
    "directory = './july_data/Ms_EW'\n",
    "\n",
    "# To ensure proper ordering of data, first need to run a file type check\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list = sorted(file_list)\n",
    "\n",
    "# Data holder\n",
    "full_index = pd.date_range(start=start_time, end=end_time, freq='s')\n",
    "minutes = pd.Series(index = full_index)\n",
    "\n",
    "# For tracking progress\n",
    "counter = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        timestamps, data = pull_record(file_path)  \n",
    "\n",
    "        timestamp_floors, interpolated_data = get_interpolated_seconds_data(timestamps, data)\n",
    "        \n",
    "        for i, ts in enumerate(timestamp_floors):\n",
    "            minutes[ts] = interpolated_data[i]\n",
    "        \n",
    "        # Delete the file after reading its content\n",
    "        #os.remove(file_path)\n",
    "\n",
    "        # Update counter for progress tracking\n",
    "        counter += 1\n",
    "        print(f'Completed {counter} of {len(file_list)} records', end = '\\r')\n",
    "\n",
    "# Now index the data to the new timestamps\n",
    "truncated_data = minutes.loc[start_time:end_time]\n",
    "\n",
    "truncated_data = truncated_data.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(truncated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89315e4a",
   "metadata": {},
   "source": [
    "## All Directories, for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81997e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=10, ncols=1, figsize=(10, 30))  # Adjust figsize as needed #x#\n",
    "\n",
    "for i, directory in enumerate(directories):\n",
    "    # To ensure proper ordering of data, first need to run a file type check\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list = sorted(file_list)\n",
    "    \n",
    "    # Data holder\n",
    "    full_index = pd.date_range(start=start_time, end=end_time, freq='s')\n",
    "    minutes = pd.Series(index = full_index)\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".h5\"):\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            timestamps, data = pull_record(file_path)  \n",
    "\n",
    "            timestamp_floors, interpolated_data = get_interpolated_seconds_data(timestamps, data)\n",
    "        \n",
    "            for j, ts in enumerate(timestamp_floors):\n",
    "                minutes[ts] = interpolated_data[j]\n",
    "\n",
    "            # Delete the file after reading its content\n",
    "            #os.remove(file_path)\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} of {len(file_list)} records', end = '\\r')\n",
    "    \n",
    "    truncated_data = minutes.loc[start_time:end_time]\n",
    "\n",
    "    truncated_data = truncated_data.interpolate(method='linear')\n",
    "    \n",
    "    # Plot the data on the corresponding subplot\n",
    "    axes[i].plot(truncated_data.values)\n",
    "    directory = directory.split('/')[2]\n",
    "    axes[i].set_title(f'Data from {directory}')\n",
    "    axes[i].set_xlabel('Seconds since 1730:00 on 26 July')\n",
    "    axes[i].set_ylabel('Reading')\n",
    "    axes[i].set_ylim(-5, 5)\n",
    "\n",
    "    #save to a single csv\n",
    "    #data_frames.to_csv('combined_data.csv', index=False)\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb06f40",
   "metadata": {},
   "source": [
    "Note: this ran really slow for the first half of the last few records then opens up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0a98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aa4489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the grid\n",
    "x, y = np.meshgrid(np.linspace(-10, 10, 20), np.linspace(-10, 10, 20))\n",
    "\n",
    "# Define the vector components\n",
    "Fx = x / np.sqrt(x**2 + y**2)\n",
    "Fy = y / np.sqrt(x**2 + y**2)\n",
    "\n",
    "# Plot the vector field\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.quiver(x, y, Fx, Fy, color='blue')\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.title('Vector Plot of F(x, y) = (x/√(x²+y²), y/√(x²+y²))')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb149ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58035f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=10, ncols=1, figsize=(10, 30))  # Adjust figsize as needed\n",
    "\n",
    "for i, directory in enumerate(directories):\n",
    "    # To ensure proper ordering of data, first need to run a file type check\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list = sorted(file_list)\n",
    "    \n",
    "    # Data holder\n",
    "    minutes = []\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".h5\"):\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            timestamps, data = pull_record(file_path)  \n",
    "\n",
    "            # For first minute get timestamp\n",
    "            if counter == 0:\n",
    "                 first_timestamp = timestamps[0].replace(microsecond=0)\n",
    "\n",
    "            minute_interpolated = get_interpolated_seconds_data(timestamps, data)\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            minutes.extend(minute_interpolated)\n",
    "\n",
    "            # Delete the file after reading its content\n",
    "            #os.remove(file_path)\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} of {len(file_list)} records', end = '\\r')\n",
    "\n",
    "    # Number of timestamps needed\n",
    "    num_timestamps = len(minutes)\n",
    "\n",
    "    # Create a list of timestamps, each one second apart\n",
    "    timestamps = [first_timestamp + timedelta(seconds=i) for i in range(num_timestamps)]\n",
    "\n",
    "    # Now index the data to the new timestamps\n",
    "    truncated_data = pd.Series(minutes, index = timestamps).loc[start_time:end_time]\n",
    "    \n",
    "    # Plot the data on the corresponding subplot\n",
    "    axes[i].plot(truncated_data.values)\n",
    "    directory = directory.split('/')[2]\n",
    "    axes[i].set_title(f'Data from {directory}')\n",
    "    axes[i].set_xlabel('Seconds since 1730:00 on 26 July')\n",
    "    axes[i].set_ylabel('Reading')\n",
    "    axes[i].set_ylim(-5, 5)\n",
    "\n",
    "    #save to a single csv\n",
    "    #data_frames.to_csv('combined_data.csv', index=False)\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f8096",
   "metadata": {},
   "source": [
    "# =-=-=-=-=-=-=-="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5761fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file1 = 'snipe_hunt_2024-07-26_15-29-26-782667.h5'\n",
    "test_file2 = 'snipe_hunt_2024-07-26_15-30-26-768969.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec18004",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timestamps1, test_data1 = pull_record(test_file1)\n",
    "test_timestamps2, test_data2 = pull_record(test_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b6288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcc5f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4218a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe4aeb2c",
   "metadata": {},
   "source": [
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9429d8d",
   "metadata": {},
   "source": [
    "### Testing for Jeopardy - By second analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seconds_data(timestamps, data):\n",
    "    '''\n",
    "    For each second's worth of samples (2597) get the key descriptive data as a downsample.\n",
    "    \n",
    "    Returns the mean, std and max difference from mean for that second - the latter useful\n",
    "    for outlier identification.\n",
    "    '''\n",
    "    output=[]\n",
    "    for i, second in enumerate(data):\n",
    "        avg = second.mean()\n",
    "        stddev = second.std()\n",
    "        mx = max(second)\n",
    "        mn = min(second)\n",
    "        max_diff = max(abs(mx-avg),abs(mn-avg))\n",
    "        \n",
    "        time = timestamps[i]\n",
    "        \n",
    "        output.append({'time':time, 'mean':avg, 'std':stddev, 'max_diff':max_diff})\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997af8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './June24_mini_expedition/SNIPE Mini Expedition Jun 26-28_EDDIE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b79206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure proper ordering of data, first need to run a file type check\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "# Data holder\n",
    "minutes = []\n",
    "\n",
    "# For tracking progress\n",
    "counter = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        timestamps, data = pull_record(file_path)\n",
    "\n",
    "        minute = get_seconds_data(timestamps, data)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        minutes.extend(minute)\n",
    "        \n",
    "        # Delete the file after reading its content\n",
    "        #os.remove(file_path)\n",
    "\n",
    "        # Update counter for progress tracking\n",
    "        counter += 1\n",
    "        print(f'Completed {counter} of {len(file_list)} records', end = '\\r')\n",
    "\n",
    "#save to a single csv\n",
    "#data_frames.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b8e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(minutes).set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd24d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[55:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out alignment times\n",
    "# Do our three sites\n",
    "# Add on those two additional graphs showing the distributions to the right\n",
    "# Align to times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea4ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe44c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snipe_analysis import *\n",
    "sample_rate = 2597\n",
    "# directory = './June24_mini_expedition/SNIPE Mini Expedition Jun 26-28_EDDIE' # Test data from June 2024\n",
    "# calibration_data_file = 'calibrationN691.csv'\n",
    "\n",
    "# Test 2\n",
    "directory = './Aux_East&Down_Jul24' # Test data from June 2024\n",
    "calibration_data_file = 'calibrationN761.csv'\n",
    "\n",
    "stddev_cutoff = 2 # <-- Change this if desired, the lower the number the less potential noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = by_second_descriptive_stats(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38cb7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16954f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee816701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = by_minute_descriptive_stats(directory)\n",
    "plt.plot(np.arange(0, len(df_m)),df_m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cut = 900 \n",
    "end_cut = start_cut + (24 * 60)\n",
    "df_m_cut = df_m[start_cut:end_cut]\n",
    "\n",
    "df_m_clean_indices = outlier_filter(df_m_cut, stddev_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the frequency bins using the sample rate\n",
    "freqs = np.fft.fftfreq(60*sample_rate, 1/sample_rate)[:(60*sample_rate)//2] \n",
    "\n",
    "# To ensure proper ordering of data\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ebecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data holder as an array of zero values\n",
    "fft_avg1 = np.zeros(60*sample_rate, dtype = 'complex128')\n",
    "# Initialize data holder as an array of zero values\n",
    "psd_avg1 = np.zeros(30*sample_rate)\n",
    "\n",
    "\n",
    "# For tracking progress\n",
    "counter = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        timestamps, data = pull_record(file_path)\n",
    "\n",
    "        # Check to see if it is in the filter\n",
    "        if timestamps[0] in df_m_clean_indices:   \n",
    "\n",
    "            data = flatten_data(data)\n",
    "\n",
    "            n = len(data)\n",
    "\n",
    "            # Compute the Fast Fourier Transform (FFT)\n",
    "            fft_result = np.fft.fft(data)\n",
    "            \n",
    "            fft_avg1 += fft_result\n",
    "\n",
    "            # Calculate the one-sided power spectral density\n",
    "            psd = (1 / (sample_rate * n)) * np.abs(fft_result[:n//2])**2\n",
    "            \n",
    "            psd = np.sqrt(2*psd)\n",
    "            \n",
    "            # This line of code aggregates\n",
    "            psd_avg1 += psd     #calibrated_psd\n",
    "\n",
    "        # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} files', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(psd_avg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a9b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstart = 25\n",
    "pstop = 275\n",
    "plt.yscale('log')\n",
    "plt.plot(freqs,psd_avg1)\n",
    "#plt.plot(freqs[pstart:pstop],psd_avg1[pstart:pstop])\n",
    "plt.xlabel('Frequency, Hz')\n",
    "plt.ylabel('PSD (log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a53c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data = np.loadtxt(calibration_data_file, delimiter = \",\")\n",
    "\n",
    "frequency_calibration = calibration_data[:, 0]  # Frequency values in Hz\n",
    "voltage_calibration = 10**-3 * calibration_data[:, 1]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "phase_calibration = calibration_data[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "voltage_calibration_interpolated = np.interp(np.abs(freqs), frequency_calibration, voltage_calibration)\n",
    "#phase_correction_interpolated = np.interp(np.abs(freqs), frequency_calibration, phase_calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877898d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_freq_domain = psd_avg1 / voltage_calibration_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b86c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(calibrated_freq_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstart = 0\n",
    "pstop = 10\n",
    "plt.yscale('log')\n",
    "#plt.plot(freqs,calibrated_freq_domain)\n",
    "plt.plot(freqs[pstart:pstop],calibrated_freq_domain[pstart:pstop])\n",
    "plt.xlabel('Frequency, Hz')\n",
    "plt.ylabel('PSD (log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c60f91",
   "metadata": {},
   "source": [
    "We get a spike every 120 Hz interval, beginning with 60Hz.  Although by 1020Hz it seems to fade into the background.\n",
    "\n",
    "We also have one at 50 Hz.\n",
    "\n",
    "Below 50Hz, we have two small peaks around 35 and 36 Hz each; bump around 8.5.  Also some rolling increase around 14 and 10 Hz. \n",
    "\n",
    "Small bump at 1.75Hz.  The energy of 1.75 Hz is approximately 7.24x10^-15 eV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tracking progress\n",
    "counter = 0\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        timestamps, data = pull_record(file_path)\n",
    "\n",
    "        # Check to see if it is in the filter\n",
    "        if timestamps[0] in df_m_clean_indices:   \n",
    "\n",
    "            data = flatten_data(data)\n",
    "\n",
    "            n = len(data)\n",
    "\n",
    "            # Compute the Fast Fourier Transform (FFT)\n",
    "            fft_result = np.fft.fft(data)\n",
    "\n",
    "            # Calculate the one-sided power spectral density\n",
    "            psd = (1 / (sample_rate * n)) * np.abs(fft_result[:n//2])**2\n",
    "\n",
    "            # Now Calibrate Here!\n",
    "            calibrated_psd = calibrate_data(freqs, psd, calibration_data_file)\n",
    "\n",
    "            # This line of code aggregates\n",
    "            psd_avg += calibrated_psd\n",
    "\n",
    "        # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} files', end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed3708",
   "metadata": {},
   "source": [
    "###### Finding the Spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "pstart = 100\n",
    "pstop = 110\n",
    "plt.yscale('log')\n",
    "#plt.plot(freqs,calibrated_freq_domain)\n",
    "plt.plot(freqs[pstart:pstop],calibrated_freq_domain[pstart:pstop])\n",
    "plt.xlabel('Frequency, Hz')\n",
    "plt.ylabel('PSD (log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_data(freqs, psd_agg, calibration_data_file):\n",
    "    '''\n",
    "    Takes in the aggregated psd data and frequencies and applies the calibration data to it.\n",
    "    '''\n",
    "    calibration_data = np.loadtxt(calibration_data_file, delimiter = \",\")\n",
    "    \n",
    "    frequency_calibration = calibration_data[:, 0]  # Frequency values in Hz\n",
    "    voltage_calibration = 10**-3 * calibration_data[:, 1]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "    phase_calibration = calibration_data[:, 2]\n",
    "\n",
    "    voltage_calibration_interpolated = np.interp(np.abs(freqs), frequency_calibration, voltage_calibration)\n",
    "    phase_correction_interpolated = np.interp(np.abs(freqs), frequency_calibration, phase_calibration)\n",
    "\n",
    "    calibrated_freq_domain = calibrate_frequency_domain(psd_agg, freqs, voltage_calibration_interpolated, phase_correction_interpolated)\n",
    "    \n",
    "    # Convert all to real here\n",
    "    calibrated_freq_domain = [x.real for x in calibrated_freq_domain]\n",
    "    \n",
    "    return calibrated_freq_domain\n",
    "\n",
    "\n",
    "def calibrate_frequency_domain(freq_domain, freqs, voltage_calibration, phase_correction):\n",
    "    '''\n",
    "    Calibrates the frequency domain transformation of the time domain data using magnetometer data.\n",
    "    '''\n",
    "    calibrated_freq_domain = np.zeros_like(freq_domain, dtype=complex)\n",
    "    for i, freq in enumerate(freqs):\n",
    "        if freq >= 0:  # Only process positive frequencies\n",
    "            calibration_factor = voltage_calibration[i]\n",
    "            phase = phase_correction[i]\n",
    "\n",
    "            # Apply calibration factor and phase correction\n",
    "            # the 2* is for one sided freq>0\n",
    "            calibrated_freq_domain[i] = 2 * freq_domain[i] / calibration_factor * np.exp(1j * phase)\n",
    "    return calibrated_freq_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9fa11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b903fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c14746f",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1846f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds from (1970/01/01 00:00:00.0) to gps epoch (1980/01/06 00:00:19.0), ignoring leap-seconds\n",
    "gpsEpoch = 315964819.\n",
    "# Constants\n",
    "sample_rate = 2597\n",
    "rho = 6.04e7 # in nT^2 dark matter density in magnetic field units\n",
    "R = 0.0212751 # in Hz^-1 Radius of earth divided by c\n",
    "fd = 1 / 86164 # in Hz, rotation frequency of the Earth (1/day)\n",
    "dT = 1/sample_rate # sampling period (in s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c5c65",
   "metadata": {},
   "source": [
    "## Needed Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca767974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pull a file (record), each of which represents 1 minute of data at 2597 samples for each of the 60 seconds:\n",
    "def pull_record(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from .h5 file.  Sets timestamps as the index in datetime format\n",
    "    for a Pandas DataFrame where each column represents a second of data and the rows are the\n",
    "    2597 individual samples for each of those seconds in chronological order.\n",
    "    '''\n",
    "    index = pd.Series(np.array(h5py.File(filename)['timestamps']))\n",
    "    df = pd.DataFrame(np.array(h5py.File(filename)['data']))\n",
    "    re_index = []\n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format\n",
    "        d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        re_index.append(d)\n",
    "    df.columns = re_index\n",
    "    return df\n",
    "\n",
    "# 2. Get descriptive statistics for each minute (record) of data:\n",
    "def get_minute_data(timestamps, data):\n",
    "    '''\n",
    "    Downsamples records all the way to the minute, returning the mean, std and max difference\n",
    "    from the mean within that minute of data.\n",
    "    '''\n",
    "    time = timestamps[0]\n",
    "    \n",
    "    avg = data.mean()\n",
    "    mx = max(data)\n",
    "    mn = min(data)\n",
    "    stddev = np.std(data)\n",
    "    max_diff = max(abs(mx-avg),abs(mn-avg))\n",
    "    \n",
    "    output = {'time':time, 'mean':avg, 'std':stddev, 'max_diff':max_diff}\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 3. Create dataframe of each minute to help identify outliers (noise):\n",
    "def by_minute_descriptive_stats(directory, store = False):\n",
    "    # List to store DataFrames\n",
    "    df_m = pd.DataFrame()\n",
    "\n",
    "    # To ensure proper ordering of data\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 1\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".h5\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            record = pull_record(file_path)\n",
    "\n",
    "            minute = get_minute_data(record)\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            df_m = pd.concat([df_m, minute], axis = 0)\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} of {len(file_list) + 1} files', end = '\\r')\n",
    "\n",
    "    # OPTIONAL: save to a single csv\n",
    "    if store == True:\n",
    "        data_frames.to_csv(f'minute_data_{directory[-6:-1]}.csv', index=False)\n",
    "        \n",
    "    return df_m\n",
    "\n",
    "# 4. Flatten each minute into one continuous series of data, with timestamps optional:\n",
    "# ! - Note: Sample rate was hard coded in here but can be changed; when timestamps option its not used anyway\n",
    "def flatten_record(df, sample_rate = 2597, time_stamp = False):\n",
    "    '''\n",
    "    Create a Pandas series where the index is the full sample timestamp\n",
    "    '''\n",
    "    \n",
    "    flattened_list = df.to_numpy().flatten(order='F').tolist()\n",
    "    \n",
    "    if time_stamp == True:\n",
    "        time_stamps = []\n",
    "        for i in range(len(df.columns)-1):\n",
    "            interval = (df.columns[i+1] - df.columns[i])/sample_rate\n",
    "            for j in range(sample_rate):\n",
    "                time_stamps.append(df.columns[i] + (interval * j))\n",
    "        # Add in the last second using the interval from the previous\n",
    "        for j in range(sample_rate):\n",
    "            time_stamps.append(df.columns[-1] + (interval * j))\n",
    "        output = pd.Series(flattened_list, index = time_stamps)\n",
    "    else:\n",
    "        output = pd.Series(flattened_list)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 5.  Pulls only those records deemed non-outliers\n",
    "def time_domain_clean(directory, good_records, store = False):\n",
    "    time_series = []\n",
    "\n",
    "    # To ensure proper ordering of data\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 1\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".h5\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            minute = pull_record(file_path)\n",
    "\n",
    "            # Check if minute is in good data\n",
    "            if minute.columns[0] in good_records:\n",
    "                series = flatten_record(minute) # Can change sample rate and if timestamps are used\n",
    "                time_series.append(series)\n",
    "\n",
    "            # OPTIONAL: Delete the file after reading its content\n",
    "    #         if delete == True:\n",
    "    #             os.remove(file_path)\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            print(f'Completed {counter} files', end = '\\r')\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "    # OPTIONAL: save to a single csv FIX THIS!\n",
    "    if store == True:\n",
    "        pd.Series(time_series).to_csv(f'fft_data_{directory[-6:-1]}.csv', index=False)\n",
    "        \n",
    "    return time_series\n",
    "\n",
    "# 5. Calibrates the frequency domain based on magnetometer calibration data\n",
    "def calibrate_frequency_domain(freq_domain, freqs, calibration_factors, phase_data):\n",
    "    calibrated_freq_domain = np.zeros_like(freq_domain, dtype=complex)\n",
    "    for i, freq in enumerate(freqs):\n",
    "        if freq >= 0:  # Only process positive frequencies\n",
    "            calibration_factor = calibration_factors[i]\n",
    "            phase = phase_data[i]\n",
    "\n",
    "            # Apply calibration factor and phase correction\n",
    "            # the 2* is for one sided freq>0\n",
    "            calibrated_freq_domain[i] = 2 * freq_domain[i] / calibration_factor * np.exp(1j * phase)\n",
    "    return calibrated_freq_domain\n",
    "\n",
    "# 6. \n",
    "def compute_noise_spectral_density(time_series, sampling_rate):\n",
    "    # Calculate the length of the time series\n",
    "    n = len(time_series)\n",
    "\n",
    "    # Compute the Fast Fourier Transform (FFT)\n",
    "    fft_result = np.fft.fft(time_series)\n",
    "\n",
    "    # Calculate the one-sided power spectral density\n",
    "    psd = (1 / (sampling_rate * n)) * np.abs(fft_result[:n//2])**2\n",
    "\n",
    "    # Calculate the corresponding frequencies\n",
    "    freqs = np.fft.fftfreq(n, 1/sampling_rate)[:n//2]\n",
    "\n",
    "    return freqs, np.sqrt(psd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca82118",
   "metadata": {},
   "source": [
    "## Determining Data to Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88e3d6",
   "metadata": {},
   "source": [
    "##### >>> LEMMY/EDDIE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a793a",
   "metadata": {},
   "source": [
    "#### 1. Load Minute Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ebced",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory1 = './June24_mini_expedition/SNIPE Mini Expedition Jun 26-28_EDDIE'\n",
    "df_m1 = by_minute_descriptive_stats(directory1)\n",
    "df_m1.info(), df_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94efbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21:40 = 20 hours 100 minutes\n",
    "#1:55 = 1 hours 55 minutes\n",
    "#19 hours, 45 minutes\n",
    "19*60+45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m1[1170:1220]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8340eb46",
   "metadata": {},
   "source": [
    "#### 2. Use 'start' and 'stop' below to cut off bad data at the beginning and end (iterative process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop = 0, len(df_m1)\n",
    "#start, stop = 300, 400\n",
    "for i in df_m1:\n",
    "    plt.plot(list(df_m1[i][start:stop]), label = i)\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50e0a9",
   "metadata": {},
   "source": [
    "#### 3. Make the cut and get descriptive statistics for outlier identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m1_cut = df_m1[start:stop]\n",
    "df_m1_cut_ds = df_m1_cut.describe()\n",
    "df_m1_cut_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb64a1d",
   "metadata": {},
   "source": [
    "#### 4. Using the descriptive statistics to set outlier filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf88c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = df_m1_cut_ds.loc['mean','std']\n",
    "std_std = df_m1_cut_ds.loc['std','std']\n",
    "\n",
    "max_diff_mean = df_m1_cut_ds.loc['mean','max_diff']\n",
    "max_diff_std = df_m1_cut_ds.loc['std','max_diff']\n",
    "\n",
    "# std_limit is how many standard deviations from the mean to set the cutoff at\n",
    "std_limit = 2\n",
    "\n",
    "std_cutoff = std_mean + (std_limit * std_std)\n",
    "max_diff_cutoff = max_diff_mean + (std_limit * max_diff_std)\n",
    "\n",
    "print(f'Max Difference Cutoff = {max_diff_cutoff},\\nStandard Deviation Cutoff = {std_cutoff}')\n",
    "\n",
    "df_m1_clean = df_m1_cut[df_m1_cut['std'] < std_cutoff]\n",
    "df_m1_clean = df_m1_clean[df_m1_clean['max_diff'] < max_diff_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_records1 = df_m1_clean.index\n",
    "len(good_records1)\n",
    "# del df_m # This deletes the minute data to free up RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a60a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_N = time_domain_clean(directory1, good_records1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa97e26",
   "metadata": {},
   "source": [
    "##### >>> PHIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12468509",
   "metadata": {},
   "source": [
    "#### 1. Load Minute Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory2 = './June24_mini_expedition/SNIPE Mini Expedition JUN 26-28_PHILL'\n",
    "df_m2 = minute_data_analysis(directory2)\n",
    "df_m2.info(), df_m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68a758",
   "metadata": {},
   "source": [
    "#### 2. Use 'start' and 'stop' below to cut off bad data at the beginning and end (iterative process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78e62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, stop = 0, len(df_m)\n",
    "start, stop = 300, 2100\n",
    "for i in df_m2:\n",
    "    plt.plot(list(df_m2[i][start:stop]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dcee4",
   "metadata": {},
   "source": [
    "#### 3. Make the cut and get descriptive statistics for outlier identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d51b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m2_cut = df_m2[start:stop]\n",
    "df_m2_cut.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d65bf2",
   "metadata": {},
   "source": [
    "#### 4. Using the descriptive statistics to set outlier filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1804128",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = df_m1_cut_ds.loc['mean','std']\n",
    "std_std = df_m1_cut_ds.loc['std','std']\n",
    "\n",
    "max_diff_mean = df_m1_cut_ds.loc['mean','max_diff']\n",
    "max_diff_std = df_m1_cut_ds.loc['std','max_diff']\n",
    "\n",
    "# std_limit is how many standard deviations from the mean to set the cutoff at\n",
    "std_limit = 2\n",
    "\n",
    "std_cutoff = std_mean + (std_limit * std_std)\n",
    "max_diff_cutoff = max_diff_mean + (std_limit * max_diff_std)\n",
    "\n",
    "print(f'Max Difference Cutoff = {max_diff_cutoff},\\nStandard Deviation Cutoff = {std_cutoff}')\n",
    "\n",
    "df_m2_clean = df_m2_cut[df_m2_cut['std'] < std_cutoff]\n",
    "df_m2_clean = df_m2_clean[df_m2_clean['max_diff'] < max_diff_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_records2 = df_m2_clean.index\n",
    "len(good_records2)\n",
    "# del df_m # This deletes the minute data to free up RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d84945",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_E = time_domain_clean(directory2, good_records2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53592c9b",
   "metadata": {},
   "source": [
    "#### 5. Transform time to frequency domain and calibrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "BdataN = time_series_N\n",
    "BdataE = time_series_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lemmy and phil data, get coords for our station, calibrate data after FFT\n",
    "\n",
    "#convert coordinates into radians\n",
    "[latL,longL] = (39 + 6.024/60)*np.pi/180, (120 + 55.386/60)*np.pi/180\n",
    "[latP, longP] = (39 + 6.101/60)*np.pi/180, (120 + 55.426/60)*np.pi/180\n",
    "\n",
    "#convert to spherical coordinates for theta\n",
    "latL = np.pi/2 - latL;\n",
    "LatP = np.pi/2 - latP;\n",
    "\n",
    "lat1 = latL\n",
    "long1 = longP\n",
    "\n",
    "##BELOW IS NOT NECESSARY FOR OUR ANALYSIS METHOD\n",
    "\n",
    "# #Range of frequencies analyzed\n",
    "# lowFreq,highFreq = [0.1,5.0];\n",
    "\n",
    "# N = len(BdataN)\n",
    "# fp = int(lowFreq*N*dT) # this picks out the data point in our list corresponding to lowFreq = Ntot LowFreq/(frequency span)\n",
    "# fq = int(highFreq*N*dT)+1;\n",
    "\n",
    "\n",
    "# # Construct data vector\n",
    "# fdhat = int(np.round(fd * N * dT)) # fdhat index. fdhat is closest discrete frequency interval to fd (fd = 1/day)\n",
    "\n",
    "##\n",
    "\n",
    "FFT1 = -dT*np.fft.fft(BdataN)\n",
    "FFT2 = dT*np.fft.fft(BdataE)\n",
    "\n",
    "calibrationL = 'LEMMYCalibration691text.csv'\n",
    "calibrationP = 'PHILCalibration748text.csv'\n",
    "\n",
    "freqsN, noise_spectral_densityN = compute_noise_spectral_density(BdataN, sample_rate)\n",
    "freqsE, noise_spectral_densityE = compute_noise_spectral_density(BdataE, sample_rate)\n",
    "\n",
    "calibration_dataL = np.loadtxt(calibrationL, delimiter = \",\")\n",
    "calibration_dataP = np.loadtxt(calibrationP, delimiter = \",\")\n",
    "\n",
    "frequency_calibrationL = calibration_dataL[:, 0]  # Frequency values in Hz\n",
    "voltage_calibrationL = 10**-3 * calibration_dataL[:, 1]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "phase_calibrationL = calibration_dataL[:,2]\n",
    "\n",
    "# FIX THESE BELOW!\n",
    "frequency_calibrationP = calibration_dataL[:, 0]  # Frequency values in Hz\n",
    "voltage_calibrationP = 10**-3 * calibration_dataP[:, 0]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "phase_calibrationP = calibration_dataP[:,1]\n",
    "# FIX THE ABOVE\n",
    "\n",
    "# calculates the calibration factor at every frequency in FFT of our data\n",
    "voltage_calibration_interpolatedN = np.interp(np.abs(freqsN), frequency_calibrationL, voltage_calibrationL)\n",
    "phase_correction_interpolatedN = np.interp(np.abs(freqsN), frequency_calibrationL, phase_calibrationL)\n",
    "voltage_calibration_interpolatedE = np.interp(np.abs(freqsE), frequency_calibrationP, voltage_calibrationP)\n",
    "phase_correction_interpolatedE = np.interp(np.abs(freqsE), frequency_calibrationP, phase_calibrationP)\n",
    "\n",
    "calFFT1 = calibrate_frequency_domain(FFT1, freqsN, voltage_calibration_interpolatedN, phase_correction_interpolatedN)\n",
    "calFFT2 = calibrate_frequency_domain(FFT2, freqsE, voltage_calibration_interpolatedE, phase_correction_interpolatedE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea56131",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STOP HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8af6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ed4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of complete FT data at Compton frequency + sidebands\n",
    "X = np.stack([calFFT1[fp - fdhat : fq - fdhat], calFFT2[fp - fdhat : fq - fdhat],\n",
    "              calFFT1[fp : fq], calFFT2[fp : fq],\n",
    "              calFFT1[fp + fdhat : fq + fdhat], calFFT2[fp + fdhat : fq + fdhat]])\n",
    "\n",
    "# Compute expectation value\n",
    "# Discrete sampling correction for sidebands\n",
    "Q = lambda f: (1 - np.exp(-2 * np.pi * 1j * f * N * dT)) / (1 - np.exp(-2 * np.pi * 1j * f * dT))\n",
    "\n",
    "# basis vectors for dark matter signal\n",
    "mu0 = -N * dT * np.sqrt(rho / 2) * np.array([\n",
    "    0, 0, 0, np.sin(lat1), 0, 0])\n",
    "muplus = -dT * np.sqrt(rho) / 2 * np.array([\n",
    "    1j * np.exp(-1j * long1) * Q(fd - fdhat / N / dT), np.cos(lat1) * np.exp(-1j * long1) * Q(fd - fdhat / N / dT),\n",
    "    1j * np.exp(-1j * long1) * Q(fd), np.cos(lat1) * np.exp(-1j * long1) * Q(fd),\n",
    "    1j * np.exp(-1j * long1) * Q(fd + fdhat / N / dT), np.cos(lat1) * np.exp(-1j * long1) * Q(fd + fdhat / N / dT)])\n",
    "muminus = -dT * np.sqrt(rho) / 2 * np.array([\n",
    "    -1j * np.exp(1j * long1) * Q(fd - fdhat / N / dT), np.cos(lat1) * np.exp(1j * long1) * Q(fd - fdhat / N / dT),\n",
    "    -1j * np.exp(1j * long1) * Q(fd), np.cos(lat1) * np.exp(1j * long1) * Q(fd),\n",
    "    -1j * np.exp(1j * long1) * Q(fd + fdhat / N / dT), np.cos(lat1) * np.exp(1j * long1) * Q(fd + fdhat / N / dT)])\n",
    "# Note that mu, nu, and S are all missing a factor of frequency, so that they can be frequency independent.  This factor will be added back when computing the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd097595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute covariance matrix\n",
    "Sigma = np.sum(X[2:4, None] * np.conj(X[2:4]), axis = -1) / (fq - fp)\n",
    "\n",
    "inva = np.linalg.inv(np.linalg.cholesky(Sigma))\n",
    "\n",
    "invA = np.block([[inva, np.zeros((2, 2)), np.zeros((2, 2))],\n",
    "                 [np.zeros((2, 2)), inva, np.zeros((2, 2))],\n",
    "                 [np.zeros((2, 2)), np.zeros((2, 2)), inva]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b5d3c",
   "metadata": {},
   "source": [
    "# ============================================================= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d504a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b11f6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bf787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffbe9bd9",
   "metadata": {},
   "source": [
    "# SCRAP WORK/APPENDICES BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15880d1e",
   "metadata": {},
   "source": [
    "### 1. Pull the timestamps as well as the data from a single record file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: H5PY file has two subtypes: 'data' and 'timestamps'\n",
    "# Given a record file = 60 seconds of 2597 samples of data\n",
    "# Returns np.array for timestamps (60) and data (60x2597).  Note data is Transposed from original format (2597x60)\n",
    "file = 'a1.h5'\n",
    "\n",
    "index = np.array(h5py.File(file)['timestamps'])\n",
    "data = np.array(h5py.File(file)['data']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185b6a8",
   "metadata": {},
   "source": [
    "### 2. Change form of index from S26 to string (or datetime later if it makes sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_index = []\n",
    "for i in index:    \n",
    "    # Decode the byte string to a regular string\n",
    "    d = i.decode('utf-8')\n",
    "\n",
    "    # Convert the string to datetime format\n",
    "    d = datetime.strptime(d, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    re_index.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7326326",
   "metadata": {},
   "source": [
    "### 3. Add the time as the column header for each second in the dataset (itself containing 2597 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab75f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = re_index\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e20e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the data is represented as 60 seconds in the columns, with the row data representing the nth-indexed sample of that second\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a1f34",
   "metadata": {},
   "source": [
    "##### Showing all samples on a single timeline, along with means for each second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f6bfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean = df.mean()\n",
    "minute = []\n",
    "for i in df.columns:\n",
    "    for l in df[i]:\n",
    "        minute.append(l)\n",
    "plt.plot(minute)\n",
    "\n",
    "# Need to rescale x axis for the mean to show it on same plot\n",
    "x_vals = np.linspace(0,60*2597,60)\n",
    "plt.plot(x_vals, mean,color='black')\n",
    "plt.xlabel('Sample #')\n",
    "plt.ylabel('V')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00728040",
   "metadata": {},
   "source": [
    "### 4. Combine the above to pull data from a single file ======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63875899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_record(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from .h5 file.  Cleans the timestamps and sets it as the index\n",
    "    for a Pandas DataFrame where each column represents a second of data and the rows are the\n",
    "    2597 individual samples for each of those seconds in chronological order.\n",
    "    \n",
    "    Note: The function Transposes the data array to make it easier for manipulation.\n",
    "    '''\n",
    "    index = pd.Series(np.array(h5py.File(filename)['timestamps']))\n",
    "    df = pd.DataFrame(np.array(h5py.File(filename)['data'])) \n",
    "    re_index = []\n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format\n",
    "        d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        re_index.append(d)\n",
    "    df.columns = re_index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pull_record_3(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from .h5 file.  Cleans the timestamps and sets it as the index\n",
    "    for a Pandas DataFrame where each column represents a second of data and the rows are the\n",
    "    2597 individual samples for each of those seconds in chronological order.\n",
    "    \n",
    "    Note: The function Transposes the data array to make it easier for manipulation.\n",
    "    '''\n",
    "    index = np.array(h5py.File(filename)['timestamps'])\n",
    "    data = np.array(h5py.File(filename)['data']).T # See note above\n",
    "    \n",
    "    timestamps = []\n",
    "    \n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format\n",
    "        d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        timestamps.append(d)\n",
    "        \n",
    "    timestamps = extrapolate_timestamps(timestamps, sample_rate = 2597)\n",
    "    \n",
    "    data = flatten_data(data)\n",
    "        \n",
    "    Series = pd.Series(data, index = timestamps)\n",
    "    \n",
    "    return Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b24107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_record(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from .h5 file.  Cleans the timestamps and sets it as the index\n",
    "    for a Pandas DataFrame where each column represents a second of data and the rows are the\n",
    "    2597 individual samples for each of those seconds in chronological order.\n",
    "    \n",
    "    Note: The function Transposes the data array to make it easier for manipulation.\n",
    "    '''\n",
    "    index = np.array(h5py.File(filename)['timestamps'])\n",
    "    data = np.array(h5py.File(filename)['data']).T # See note above\n",
    "    \n",
    "    timestamps = []\n",
    "    \n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format\n",
    "        d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        timestamps.append(d)\n",
    "    \n",
    "    return timestamps, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3bc3e",
   "metadata": {},
   "source": [
    "2597 samples per second, 60 seconds per minute = 155,820 samples per minute (also datapoints per record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9eeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_timestamps(timestamps, sample_rate = 2597):\n",
    "    '''\n",
    "    Create an np array with every interpolated timestamp for each sample within the minute\n",
    "    '''\n",
    "    \n",
    "    time_stamps = []\n",
    "    for i in range(len(timestamps)-1):\n",
    "        interval = (timestamps[i+1] - timestamps[i])/sample_rate\n",
    "        for j in range(sample_rate):\n",
    "                time_stamps.append(timestamps[i] + (interval * j))\n",
    "    # Add in the last second using the interval from the previous\n",
    "    for k in range(sample_rate):\n",
    "        time_stamps.append(timestamps[-1] + (interval * j))\n",
    "        \n",
    "    return np.array(time_stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(data):\n",
    "    '''\n",
    "    Takes in a 2D array of time data (seconds, samples) and converts it to one long array (samples).\n",
    "    \n",
    "    Ensure the data is properly transposed!\n",
    "    '''\n",
    "    \n",
    "    return data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pull_record('a1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Series = flatten_record(df, sample_rate = 2597, time_stamp = True)\n",
    "Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509de4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Series = test_pull_record_3('sample_record.h5')\n",
    "Series.nbytes, Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps2, data2 = test_pull_record_2('sample_record.h5')\n",
    "timestamps2 = extrapolate_timestamps(timestamps2, sample_rate = 2597)\n",
    "data2 = flatten_data(data2)\n",
    "Series2 = pd.Series(data2, index = timestamps2)\n",
    "Series2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Series2.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Series2, columns = ['V'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_minute_data(filename):\n",
    "    '''\n",
    "    Returns an np.array with all timestamps indexed to another np.array with all the data readings at those timestamps.\n",
    "    '''\n",
    "    timestamps, data = pull_record(filename)\n",
    "    \n",
    "    timestamps = extrapolate_timestamps(timestamps, sample_rate = 2597)\n",
    "    data = flatten_data(data)\n",
    "    \n",
    "    return pd.Series(data, index = timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,d = pull_record(file)\n",
    "d = flatten_data(d)\n",
    "get_minute_data(t,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964cb562",
   "metadata": {},
   "source": [
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f35e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'sample_record.h5'\n",
    "pull_record(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2,3]\n",
    "B = [3,4,5]\n",
    "C = [5,6,7]\n",
    "\n",
    "pd.Series(A, index = B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bcffdf",
   "metadata": {},
   "source": [
    "### 5. Flatten records to timestamp each measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2afc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Flatten the DataFrame column by column\n",
    "flattened_list = df.to_numpy().flatten(order='F').tolist()\n",
    "\n",
    "print(flattened_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_record(df, sample_rate = 2597, time_stamp = False):\n",
    "    '''\n",
    "    Create a Pandas series where the index is the full sample timestamp\n",
    "    '''\n",
    "    \n",
    "    flattened_list = df.to_numpy().flatten(order='F').tolist()\n",
    "    \n",
    "    if time_stamp == True:\n",
    "        time_stamps = []\n",
    "        for i in range(len(df.columns)-1):\n",
    "            interval = (df.columns[i+1] - df.columns[i])/sample_rate\n",
    "            for j in range(sample_rate):\n",
    "                time_stamps.append(df.columns[i] + (interval * j))\n",
    "        # Add in the last second using the interval from the previous\n",
    "        for j in range(sample_rate):\n",
    "            time_stamps.append(df.columns[-1] + (interval * j))\n",
    "        output = pd.Series(flattened_list, index = time_stamps)\n",
    "    else:\n",
    "        output = pd.Series(flattened_list)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pull_record\n",
    "df_s = get_seconds_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d88a69c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_rate = 2597\n",
    "\n",
    "time_stamps = []\n",
    "flat_data = []\n",
    "for i in range(len(df.columns)-1):\n",
    "    interval = (df.columns[i+1] - df.columns[i])/sample_rate\n",
    "    for j in range(sample_rate):\n",
    "        time_stamps.append(df.columns[i] + (interval * j))\n",
    "# Add in the last second using the interval from the previous\n",
    "for j in range(sample_rate):\n",
    "    time_stamps.append(df.columns[-1] + (interval * j))\n",
    "\n",
    "flattened_list = df.to_numpy().flatten(order='F').tolist()\n",
    "\n",
    "output = pd.Series(flattened_list, index = time_stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(time_stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0d1ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_frames_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cbe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_record(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38873734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abb215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90481ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37fb2ef8",
   "metadata": {},
   "source": [
    "### 5. Getting descriptive data for each second on a record\n",
    "AKA Downsampling via the mean value for each second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seconds_data(df):\n",
    "    '''\n",
    "    For each second's worth of samples (2597) get the key descriptive data as a downsample.\n",
    "    \n",
    "    Returns the mean, std and max difference from mean for that second - the latter useful\n",
    "    for outlier identification.\n",
    "    '''\n",
    "    dd = df.describe().loc[['mean','std','min','max']].T\n",
    "    abs_diff_max_mean = (dd['max'] - dd['mean']).abs()\n",
    "    abs_diff_min_mean = (dd['mean'] - dd['min']).abs()\n",
    "\n",
    "    # Calculate the maximum of the absolute differences for each row\n",
    "    max_diff = []\n",
    "    for i in range(len(abs_diff_max_mean)):\n",
    "        max_diff.append(max(abs_diff_max_mean.iloc[i],abs_diff_min_mean.iloc[i]))\n",
    "\n",
    "    dd['max_diff'] = max_diff\n",
    "    df = dd[['mean','std','max_diff']]\n",
    "    \n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'snipe_hunt_2024-07-27_16-01-27-834200.h5'\n",
    "file2 = 'test_file.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_record(filename):\n",
    "    '''\n",
    "    Pulls data and timestamps from a SNIPE Magnetometer .h5 file.  Cleans the timestamps and puts them into a 1D array.\n",
    "    Puts the magnetometer reading data into a 2D array, where each column represents a second and each row one of the\n",
    "    2597 individual samples for that seconds in chronological order.\n",
    "    The function also captures the timestamp associated with the filename for possible use.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename (string): .h5 data file.  Ensure the full path is correct!\n",
    "    \n",
    "    Returns:\n",
    "    - file_timestamp (datetime): Timestamp for file (minute worth of data)\n",
    "    - timestamps (datetime array): Timestamps associated with data 'seconds'\n",
    "    - data (2D float array): Data associated with timestamps\n",
    "    \n",
    "    Note: The function Transposes the data array to make it easier for manipulation.\n",
    "    '''\n",
    "    # Extract the raw data into numpy arrays\n",
    "    index = np.array(h5py.File(filename)['timestamps'])\n",
    "    data = np.array(h5py.File(filename)['data']).T # See note above\n",
    "    \n",
    "    # This is to hold the timestamps converted into datetime format\n",
    "    timestamps = []\n",
    "    \n",
    "    # In order to accomodate both test files and files of various name formats (you lose the tru file_timestamp though)\n",
    "    try:\n",
    "        # In order to accomodate different file/directory combinations, the next line identifies the start index required\n",
    "        start_index = filename.find('2024')\n",
    "        # This next line of code extracts the minute record's timestamp, in datetime format\n",
    "        file_timestamp = datetime.strptime(filename[start_index:-3], '%Y-%m-%d_%H-%M-%S-%f')\n",
    "    # If the filename is non-standard then use the first timestamp from the GPS for the file timestamp\n",
    "    except:\n",
    "        print('Filename not in proper format')\n",
    "        file_timestamp = datetime.strptime(index[0].decode('utf-8'), '%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    # Now loop through the timestamp index and convert them to datetime format\n",
    "    for i in index:\n",
    "        # Decode the byte string to a regular string\n",
    "        decoded_string = i.decode('utf-8')\n",
    "\n",
    "        # Convert the string to datetime format (the try except was just put in to address bad data in file)\n",
    "        try:\n",
    "            d = datetime.strptime(decoded_string, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            timestamps.append(d)\n",
    "        except:\n",
    "            decoded = decoded_string[:19] + '.' + decoded_string[19:]\n",
    "            d = datetime.strptime(decoded, '%Y-%m-%d %H:%M:%S.%f')\n",
    "            timestamps.append(d)\n",
    "\n",
    "    return file_timestamp, timestamps, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_stamp, timestamps, data = pull_record(file1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c8732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_stats(timestamps, data):\n",
    "    '''\n",
    "    Downsamples record to the minute, returning the mean, std and max difference\n",
    "    from the mean magnetometer reading within that minute.\n",
    "    '''\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    for i, second in enumerate(data):\n",
    "        time = timestamps[i]\n",
    "        avg = second.mean()\n",
    "        mx = max(second)\n",
    "        mn = min(second)\n",
    "        stddev = np.std(second)\n",
    "        max_diff = max(abs(mx-avg),abs(mn-avg))\n",
    "        output.append({'time':time, 'mean':avg, 'std':stddev, 'max_diff':max_diff})\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'calibrationN761.csv'\n",
    "b = 'Coil1Calibration.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e57f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "b[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df96763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def by_second_descriptive_stats(directory, store = False):\n",
    "    '''\n",
    "    Takes in a directory of records and returns the descriptive statistics for the aggregated seconds,    \n",
    "    '''\n",
    "    # List to store DataFrames\n",
    "    seconds = []\n",
    "\n",
    "    # To ensure proper ordering of data\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "    file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "    # For tracking progress\n",
    "    counter = 1\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in file_list:\n",
    "        if filename.endswith(\".h5\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            file_timestamp, timestamps, data = pull_record(file_path)\n",
    "\n",
    "            seconds_stats = get_second_stats(timestamps, data)\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            seconds.extend(seconds_stats)\n",
    "\n",
    "            # Update counter for progress tracking\n",
    "            counter += 1\n",
    "            print(f'Completed {counter} of {len(file_list) + 1} files', end = '\\r')\n",
    "\n",
    "    # OPTIONAL: save to a single csv\n",
    "    if store == True:\n",
    "        seconds.to_csv(f'minute_data_{directory[-6:-1]}.csv', index=False)\n",
    "        \n",
    "    return pd.DataFrame(seconds).set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e58c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_second_descriptive_stats(directoryEW, store = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882ebe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453db87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21749c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = get_seconds_data(df)\n",
    "df_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7f7fb",
   "metadata": {},
   "source": [
    "7x53 gets wrapped up into 1 timestamped record.  Will get 7 samples per second and therefore 60x7 (420) samples per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d874f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minute_data(df):\n",
    "    '''\n",
    "    Downsamples records all the way to the minute, returning the mean, std and max difference\n",
    "    from the mean within that minute of data.\n",
    "    '''\n",
    "    indx = df.columns[0]\n",
    "    \n",
    "    minute_data = df.T.stack()\n",
    "    avg = minute_data.mean()\n",
    "    mx = max(minute_data)\n",
    "    mn = min(minute_data)\n",
    "    stddev = np.std(minute_data)\n",
    "    max_diff = max(abs(mx-avg),abs(mn-avg))\n",
    "    \n",
    "    output = pd.DataFrame({indx:{'mean':avg, 'std':stddev, 'max_diff':max_diff}}).T\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91054b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_minute_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f3d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df.columns)):\n",
    "        time = float(df.columns[i][-8:-1])\n",
    "        print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedd0b6",
   "metadata": {},
   "source": [
    "### 6. Code to extract all data from .h5 files in DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ff8d5",
   "metadata": {},
   "source": [
    "##### All Data - Note: Without substantial compute this is not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = sample_rate * 60 * len(file_list)\n",
    "\n",
    "timestamps = np.empty(total_records, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [1,2,3,4,5]\n",
    "i = 5\n",
    "\n",
    "timestamps[i:(i+len(t))] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8eee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "###!!! - This works but is TOO BIG for the kernel\n",
    "\n",
    "# Step 1: \n",
    "directory = './East-N149'\n",
    "\n",
    "# To ensure proper ordering of data\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "# For tracking progress\n",
    "counter = 1\n",
    "i = 0\n",
    "\n",
    "total_records = sample_rate * 60 * len(file_list)\n",
    "\n",
    "timestamps = np.empty(total_records, dtype=object)\n",
    "data = np.empty(total_records, dtype=np.float32)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        t, d = full_minute(file_path)\n",
    "        \n",
    "        timestamps[i:(i+len(t))] = t\n",
    "        data[i:(i+len(d))] = d\n",
    "        \n",
    "        # Delete the file after reading its content\n",
    "        #os.remove(file_path)\n",
    "        \n",
    "        # Update counter for progress tracking\n",
    "        print(f'Completed {counter} of {len(file_list)} files', end = '\\r')\n",
    "        counter += 1\n",
    "        i += (sample_rate * 60)\n",
    "\n",
    "#save to a single csv\n",
    "#data_frames.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3465fdfb",
   "metadata": {},
   "source": [
    "##### Downsample every N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: \n",
    "directory = './SNIPE Mini Expedition Jun 26-28_EDDIE'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# To ensure proper ordering of data\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "# For tracking progress\n",
    "counter = 1\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        minute = pull_record(file_path)\n",
    "        \n",
    "        df_m = get_minute_data(df)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        data_frames_m = pd.concat([data_frames_m,df_m], axis = 0)\n",
    "        \n",
    "        # Delete the file after reading its content\n",
    "        #os.remove(file_path)\n",
    "        \n",
    "        # Update counter for progress tracking\n",
    "        counter += 1\n",
    "        print(f'Completed {counter} of 2370 files', end = '\\r')\n",
    "\n",
    "#save to a single csv\n",
    "#data_frames.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda09f4f",
   "metadata": {},
   "source": [
    "##### By the Minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the data files\n",
    "directory = './SNIPE Mini Expedition Jun 26-28_EDDIE'\n",
    "\n",
    "# List to store DataFrames\n",
    "data_frames_m = pd.DataFrame()\n",
    "\n",
    "# To ensure proper ordering of data\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "# For tracking progress\n",
    "counter = 1\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        df = pull_record(file_path)\n",
    "        \n",
    "        df_m = get_minute_data(df)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        data_frames_m = pd.concat([data_frames_m,df_m], axis = 0)\n",
    "        \n",
    "        # Delete the file after reading its content\n",
    "        #os.remove(file_path)\n",
    "        \n",
    "        # Update counter for progress tracking\n",
    "        counter += 1\n",
    "        print(f'Completed {counter} of 2370 files', end = '\\r')\n",
    "\n",
    "#save to a single csv\n",
    "#data_frames.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73394114",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames_m.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22772b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_frames_m:\n",
    "    plt.plot(list(data_frames_m[i][200:2000]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec868b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_cut = data_frames_m[200:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f663e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_cut.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = .005015\n",
    "std_std = .006107\n",
    "\n",
    "max_diff_mean = .046910\n",
    "max_diff_std = .057105\n",
    "\n",
    "std_limit = 2\n",
    "\n",
    "std_cutoffs = std_mean + (std_limit * std_std)\n",
    "max_diff_cutoffs = max_diff_mean + (std_limit * max_diff_std)\n",
    "\n",
    "std_cutoffs, max_diff_cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df_m_cut[df_m_cut['std'] < std_cutoffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned[cleaned['max_diff'] < max_diff_cutoffs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a157da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cleaned:\n",
    "    plt.plot(list(cleaned[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cleaned.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50134999",
   "metadata": {},
   "source": [
    "##### By the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the data files\n",
    "directory = './East-N149'\n",
    "\n",
    "# List to store DataFrames\n",
    "data_frames_s = pd.DataFrame()\n",
    "\n",
    "# To ensure proper ordering of data, first need to run a file type check\n",
    "file_list = [f for f in os.listdir(directory) if f.endswith(\".h5\")]\n",
    "file_list.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)))\n",
    "\n",
    "# For tracking progress\n",
    "counter = 1\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in file_list:\n",
    "    if filename.endswith(\".h5\"):\n",
    "\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        df = pull_record(file_path)\n",
    "\n",
    "        df_s = get_seconds_data(df).T\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        data_frames_s = pd.concat([data_frames_s,df_s])\n",
    "\n",
    "        # Delete the file after reading its content\n",
    "        #os.remove(file_path)\n",
    "\n",
    "        # Update counter for progress tracking\n",
    "        counter += 1\n",
    "        print(f'Completed {counter} of {len(file_list)+1} records', end = '\\r')\n",
    "\n",
    "#save to a single csv\n",
    "#data_frames.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames_s.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049af533",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_frames_s:\n",
    "    plt.plot(list(data_frames_s[i]), label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames_s[60320:60380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e285c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "491d46d8",
   "metadata": {},
   "source": [
    "### 7. Outlier Analysis Step 1 - Identify best chunk of continuous data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d5e64",
   "metadata": {},
   "source": [
    "We would ultimately like 48 hours of coordinated data across the (4?) SNIPE hunting grounds.  But we will be happy with 24.  For this example we have 30 hours of data we used.  Manually identified it based off of graph in part 6 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542954f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dfs as a modified data_frames_s\n",
    "dfs = data_frames_s[12000:120000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbce291",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs:\n",
    "    plt.plot(list(dfs[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7803ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441860d",
   "metadata": {},
   "source": [
    "### 8. Outlier Analysis Part 2 - Identify and replace outlier \"spikes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13851432",
   "metadata": {},
   "source": [
    "**! - Need to lock in the method**\n",
    "\n",
    "If the row has a std for max_diff greater than 0.057 (3 std above normal) which represents around 1626 records to replace.\n",
    "Loop through values and when you find one that fails it gets replaced by the interpolation between the last good value and the next good value\n",
    "\n",
    "outliers_free = []\n",
    "\n",
    "for value in records:\n",
    "    if value is good:\n",
    "        if counter = 0:\n",
    "            append to p\n",
    "         if counter > 0:\n",
    "             check the next index to make \n",
    "    if value is bad:\n",
    "        mark index of last good value\n",
    "        turn on couner\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "7 samples per second\n",
    "420 samples per minute\n",
    "check each minute for outliers\n",
    "throw out each outlier minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81668ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['std_trig'] = dfs['std'] > 0.023612\n",
    "dfs['max_diff_trig'] = dfs['max_diff'] > 0.057848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d01d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['compo'] = dfs['std_trig'] == dfs['max_diff_trig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs:\n",
    "    print(dfs[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 15, 4, 15, 6, 7, 20, 9, 2],\n",
    "    'B': [2, 3, 20, 5, 25, 7, 8, 30, 10, 3],\n",
    "    'C': [3, 4, 25, 6, 30, 8, 9, 35, 11, 4]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 10\n",
    "\n",
    "# Function to perform the interpolation\n",
    "def interpolate_values(series, threshold):\n",
    "    n = len(series)\n",
    "    below_threshold_indices = np.where(series <= threshold)[0]\n",
    "    \n",
    "    if len(below_threshold_indices) == 0:\n",
    "        return series\n",
    "    \n",
    "    # Replace values before the first below-threshold value with that value\n",
    "    first_below_threshold_index = below_threshold_indices[0]\n",
    "    series[:first_below_threshold_index] = series[first_below_threshold_index]\n",
    "    \n",
    "    # Iterate through the below-threshold indices to perform interpolation\n",
    "    for i in range(1, len(below_threshold_indices)):\n",
    "        start_idx = below_threshold_indices[i - 1]\n",
    "        end_idx = below_threshold_indices[i]\n",
    "        \n",
    "        # Linear interpolation between start_idx and end_idx\n",
    "        series[start_idx:end_idx + 1] = np.linspace(series[start_idx], series[end_idx], end_idx - start_idx + 1)\n",
    "    \n",
    "    return series\n",
    "\n",
    "# Apply the interpolation function to each column\n",
    "for column in df.columns:\n",
    "    df[column] = interpolate_values(df[column], threshold)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c623bb",
   "metadata": {},
   "source": [
    "### 9.  Run Frequency Analysis on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = list(dfs['mean'])\n",
    "timestamps = dfs.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seconds from (1970/01/01 00:00:00.0) to gps epoch (1980/01/06 00:00:19.0), ignoring leap-seconds\n",
    "gpsEpoch = 315964819.\n",
    "\n",
    "# Constants\n",
    "SampleRate = 2597 # This is the sample rate per second, although I have reduced these to the mean of every minute\n",
    "## How does the above impact things, if at all?!?!\n",
    "\n",
    "rho = 6.04e7 # in nT^2 dark matter density in magnetic field units\n",
    "R = 0.0212751 # in Hz^-1 Radius of earth divided by c\n",
    "fd = 1 / 86164 # in Hz, rotation frequency of the Earth (1/day)\n",
    "dT = 1/SampleRate # sampling period (in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fabecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_noise_spectral_density(time_series, sampling_rate):\n",
    "    # Calculate the length of the time series\n",
    "    n = len(time_series)\n",
    "\n",
    "    # Compute the Fast Fourier Transform (FFT)\n",
    "    fft_result = np.fft.fft(time_series)\n",
    "\n",
    "    # Calculate the one-sided power spectral density\n",
    "    psd = (1 / (sampling_rate * n)) * np.abs(fft_result[:n//2])**2\n",
    "\n",
    "    # Calculate the corresponding frequencies\n",
    "    freqs = np.fft.fftfreq(n, 1/sampling_rate)[:n//2]\n",
    "\n",
    "    return freqs, np.sqrt(psd)\n",
    "\n",
    "\n",
    "def calibrate_frequency_domain(freq_domain, freqs, calibration_factors, phase_data):\n",
    "    calibrated_freq_domain = np.zeros_like(freq_domain, dtype=complex)\n",
    "    for i, freq in enumerate(freqs):\n",
    "        if freq >= 0:  # Only process positive frequencies\n",
    "            calibration_factor = calibration_factors[i]\n",
    "            phase = phase_data[i]\n",
    "\n",
    "            # Apply calibration factor and phase correction\n",
    "            # the 2* is for one sided freq>0\n",
    "            calibrated_freq_domain[i] = 2 * freq_domain[i] / calibration_factor * np.exp(1j * phase)\n",
    "    return calibrated_freq_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0790b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lemmy and phil data, get coords for our station, calibrate data after FFT\n",
    "\n",
    "#convert coordinates into radians\n",
    "[latL,longL] = (39 + 6.024/60)*np.pi/180, (120 + 55.386/60)*np.pi/180\n",
    "[latP, longP] = (39 + 6.101/60)*np.pi/180, (120 + 55.426/60)*np.pi/180\n",
    "\n",
    "#convert to spherical coordinates for theta\n",
    "latL = np.pi/2 - latL;\n",
    "LatP = np.pi/2 - latP;\n",
    "\n",
    "lat1 = latL\n",
    "long1 = longP\n",
    "\n",
    "#Range of frequencies analyzed\n",
    "lowFreq,highFreq = [0.1,5.0];\n",
    "\n",
    "T = len(measurements)\n",
    "fp = int(lowFreq*T*dT) # this picks out the data point in our list corresponding to lowFreq = Ntot LowFreq/(frequency span)\n",
    "fq = int(highFreq*T*dT)+1;\n",
    "\n",
    "\n",
    "# Construct data vector\n",
    "fdhat = int(np.round(fd * T * dT)) # fdhat index. fdhat is closest discrete frequency interval to fd (fd = 1/day)\n",
    "FFT1 = -np.fft.fft(measurements)\n",
    "#FFT2 = np.fft.fft(BdataE)\n",
    "\n",
    "# os.chdir('/Users/gc2138/Desktop/SNIPE')\n",
    "# calibrationL = 'LEMMYCalibration691text.csv'\n",
    "# calibrationP = 'PHILCalibration748text.csv'\n",
    "\n",
    "freqsN, noise_spectral_densityN = compute_noise_spectral_density(measurements, SampleRate)\n",
    "#freqsE, noise_spectral_densityE = compute_noise_spectral_density(BdataE, SampleRate)\n",
    "\n",
    "# calibration_dataL = np.loadtxt(calibrationL, delimiter = \",\")\n",
    "# calibration_dataP = np.loadtxt(calibrationP, delimiter = \",\")\n",
    "\n",
    "# frequency_calibrationL = calibration_dataL[:, 0]  # Frequency values in Hz\n",
    "# voltage_calibrationL = 10**-3 * calibration_dataL[:, 1]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "# phase_calibrationL = calibration_dataL[:,2]\n",
    "\n",
    "# frequency_calibrationP = calibration_dataP[:, 0]  # Frequency values in Hz\n",
    "# voltage_calibrationP = 10**-3 * calibration_dataP[:, 1]    # Volts per nano-Tesla (original calibration data is in mV/nT)\n",
    "# phase_calibrationP = calibration_dataP[:,2]\n",
    "\n",
    "# # calculates the calibration factor at every frequency in FFT of our data\n",
    "# voltage_calibration_interpolatedN = np.interp(np.abs(freqsN), frequency_calibrationL, voltage_calibrationL)\n",
    "# phase_correction_interpolatedN = np.interp(np.abs(freqsN), frequency_calibrationL, phase_calibrationL)\n",
    "# voltage_calibration_interpolatedE = np.interp(np.abs(freqsE), frequency_calibrationP, voltage_calibrationP)\n",
    "# phase_correction_interpolatedE = np.interp(np.abs(freqsE), frequency_calibrationP, phase_calibrationP)\n",
    "\n",
    "#calFFT1 = calibrate_frequency_domain(FFT1, freqsN, voltage_calibration_interpolatedN, phase_correction_interpolatedN)\n",
    "#calFFT2 = calibrate_frequency_domain(FFT2, freqsE, voltage_calibration_interpolatedE, phase_correction_interpolatedE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495a62b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b70241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7e64d3",
   "metadata": {},
   "source": [
    "Loading all fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb579af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
